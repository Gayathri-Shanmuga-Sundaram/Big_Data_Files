{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOk95RC9VQVHU3DTx7jLUuk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fLr2KtIRadkU"},"outputs":[],"source":["# Install pyspark\n","!pip install pyspark"]},{"cell_type":"code","source":["# Import SparkSession\n","from pyspark.sql import SparkSession"],"metadata":{"id":"RmpGByiZauPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a Spark Session\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","# Check Spark Session Information\n","spark"],"metadata":{"id":"kKrF2wseayf9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import a Spark function from library\n","from pyspark.sql.functions import col"],"metadata":{"id":"IihYsDyja2LC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install findspark"],"metadata":{"id":"3I-CCEDqa3tQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <center> **M3.2 ELT and Spark SQL**</center>\n","\n","### <p style=\"color:brown;\"> Submission by Team Supreme : Raghuveer Karrotu , Vinaya Rajaram Nayak, Arivarasan Ramasamy, Gayathri Shanmuga Sundaram \n"," \n","### <p style=\"color:brown;\"> Assignment Description : \n","### <p style=\"color:brown;\">Load the data into a spark dataframe , Show the schema, and make any necessary changes to the data schema, Conduct any transformations, Store the data into a persistent table and Create a temp view of the data\n","\n","# <center> <p style=\"color:green;\"> **Holiday Package Analysis**</p>  </center>  \n","#### <p style=\"color:cyan;\">Aim: To predict which customer is more likely to purchase the newly introduced travel package so that company could make its marketing expenditure more efficient.  </p> \n","\n","#### <p style=\"color:cyan;\">Dataset Description : The dataset we use in this analysis is from Kaggle which was obtained from “Travel.com” website. Our dataset includes various customer demographics and company features like the following \n","##### CustomerID : Unique customer Id\n","##### ProdTaken : This our target variable which says whether the customer purchased the product pitched.\n","##### Age : Age of the customer \n","##### TypeofContact : How customer was contacted (Company Invited or Self Inquiry) \n","##### CityTier : City tier depends on the development of a city, population, facilities, and living standards. \n","##### DurationOfPitch : Duration of the pitch by a salesperson to the customer,\n","##### Occupation : Occupation of the customer\n","##### Gender : Gender of the customer\n","##### NumberOfPersonVisiting : Total number of persons planning to take the trip with the customer\n","##### NumberOfFollowups : Total number of follow-ups has been done by the salesperson after the sales pitch\n","##### ProductPitched : Product pitched by the salesperson\n","##### PreferredPropertyStar : Preferred hotel property rating by customer\n","##### MaritalStatus : Marital status of customer\n","##### NumberOfTrips : Average number of trips in a year by customer\n","##### Passport : The customer has a passport or not (0: No, 1: Yes)\n","##### PitchSatisfactionScore : Sales pitch satisfaction score\n","##### OwnCar : Whether the customers own a car or not (0: No, 1: Yes)\n","##### NumberOfChildrenVisiting : Total number of children with age less than 5 planning to take the trip with the customer\n","##### Designation : Designation of the customer in the current organization\n","##### MonthlyIncome : Gross monthly income of the customer\n","</p> \n"],"metadata":{"id":"H1wdTrAFmSHg"}},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession;\n","\n","spark = SparkSession.builder.config(\"spark.driver.host\",\"localhost\").master(\"local[4]\").appName(\"ISM6562 Spark Assignment App\").getOrCreate();\n","\n","# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n","sc = spark.sparkContext  \n","\n","# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n","# this spark session webUI will be on a different port than the default (4040). One way to \n","# identify this part is with the following line. If there was only one spark session running, \n","# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n","spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n","print(\"Spark Session WebUI Port: \" + spark_session_port)"],"metadata":{"id":"fqsPNP8RbV5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n","sc.setLogLevel(\"ERROR\") "],"metadata":{"id":"s85ozgSQbl5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"id":"q0xju8jUbxYc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading our data into spark dataframe. "],"metadata":{"id":"K4JjIIrqmH8V"}},{"cell_type":"code","source":["# Load CSV file\n","df_spark = spark.read.csv(\"/content/Big Data Files/Travel.csv\", header=True, inferSchema=True)\n","df_spark.show()"],"metadata":{"id":"c7jvVbPzb0gZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Rename Column "],"metadata":{"id":"t3VAcnUYmJI-"}},{"cell_type":"code","source":["df_renamed = df_spark.withColumnRenamed(\"CustomerID\",\"customer_id\").withColumnRenamed(\"ProdTaken\",\"prodtaken\").withColumnRenamed(\"Age\",\"age\")\n","df_renamed.show()"],"metadata":{"id":"LdiEbXP-iyVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Exploration and Transformations "],"metadata":{"id":"nP3R4YN8mEfo"}},{"cell_type":"code","source":["df_renamed.printSchema()"],"metadata":{"id":"xTrI1LNycSeH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing data"],"metadata":{"id":"qd8LrzwDmCIg"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Assuming df_spark is the Spark DataFrame containing the column of interest\n","# Convert Spark DataFrame to Pandas DataFrame\n","df_pandas = df_renamed.toPandas()\n","\n","sns.countplot(x='DurationOfPitch', data=df_pandas) # checking distribution of \"durationofpitch\""],"metadata":{"id":"16pibvo9cTqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Finding missing values"],"metadata":{"id":"71g_RjVtl_dh"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Find columns with missing values\n","columns_with_missing_values = [column for column in df_renamed.columns if df_renamed.filter(col(column).isNull()).count() > 0]\n","\n","# Print columns with missing values\n","print(\"Columns with missing values:\")\n","for column in columns_with_missing_values:\n","    print(column)\n"],"metadata":{"id":"Wu1HwNtPcYL3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imputing missing values"],"metadata":{"id":"qkT6tHv5l9Lu"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Group by the column and apply the count() function\n","count_df = df_renamed.groupBy(\"TypeofContact\").count()\n","\n","# Show the resulting counts\n","count_df.show()"],"metadata":{"id":"bn7y9AshcbDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_spark = df_renamed.fillna(\"Self Enquiry\", subset=[\"TypeofContact\"])"],"metadata":{"id":"3WCtKg81cbpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imputing with missing value with median value for numeric values"],"metadata":{"id":"R5oVg6uQl6N5"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","from pyspark.sql.functions import percentile_approx\n","from pyspark.sql.functions import when\n","\n","# Iterate over columns with missing values\n","for column in columns_with_missing_values:\n","    # Calculate median of the column\n","    median_value = df_spark.select(column).agg(percentile_approx(column, 0.5)).collect()[0][0]\n","    if median_value is not None:\n","    # Round median_value to nearest integer\n","        median_value_rounded = int(round(median_value))\n","    else:\n","        median_value_rounded = 0\n","    \n","    # Impute missing values with median value\n","    df_spark = df_spark.withColumn(column, when(col(column).isNull(), median_value_rounded).otherwise(col(column)))"],"metadata":{"id":"gkNAQwW7chxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# verifying if all missing values were imputed\n","columns_with_missing_values = [column for column in df_spark.columns if df_spark.filter(col(column).isNull()).count() > 0]\n","\n","# Print columns with missing values\n","print(\"Columns with missing values:\")\n","for column in columns_with_missing_values:\n","    print(column)"],"metadata":{"id":"BOOoHrP-ckwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save imputed data with original header\n","\n","df_spark.write.option(\"header\", True).csv(\"/content/Big Data Output/processed_travel_withheader.csv\")"],"metadata":{"id":"ZW3TicFzcppF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_spark.show(20)"],"metadata":{"id":"lNfLqxqUc7m9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Storing the data into a persistent table and creating a temp view of the data\n"],"metadata":{"id":"dPoi2__Zl08w"}},{"cell_type":"code","source":["# Create a database\n","spark.sql(\"CREATE DATABASE IF NOT EXISTS Travel\")\n","\n","# Use the database\n","spark.sql(\"USE Travel\")"],"metadata":{"id":"d0bgjoYXknQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store the data into a persistent table in the Travel db\n","df_spark.write.saveAsTable(\"travel_information\")"],"metadata":{"id":"_-bVO54Okud7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a temporary view of the data\n","df_spark.createOrReplaceTempView(\"travel_information_view\")"],"metadata":{"id":"hXaHacebkx8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify if the table exists in the created database\n","check = spark.sql(\"SHOW TABLES\")\n","if check.filter(check.tableName == \"travel_information_view\").count() > 0:\n","    print(\"Table exists in the created database.\")"],"metadata":{"id":"tRKG0Ms_k0CX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now let us find some insights by using the aggregation.\n","#### <p style=\"color:brown;\"> 1. The average age of customers </p>\n","<p style=\"color:brown;\">First we are interested to know the average age of customers </p>\n"],"metadata":{"id":"vhaboyjllqah"}},{"cell_type":"code","source":["avg_age_result = spark.sql(\"SELECT ROUND(AVG(Age),2) as avg_age FROM travel_information_view\").show()"],"metadata":{"id":"mmytc76Tk6AX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <p style=\"color:brown;\"> 2. Occupation wise display of customer who bought products</p>\n","<p style=\"color:brown;\">Next let us calculate  the number of customers who have taken a product in  each occupation. We calculate the total number of customers and no of customers who bought the product in each occupation.</p>\n"],"metadata":{"id":"XgsuK6U5lnto"}},{"cell_type":"code","source":["total_result = spark.sql(\"\"\"\n","    SELECT Occupation, COUNT(*) AS total_customers, SUM(ProdTaken) AS customers_purchased_product\n","    FROM travel_information_view\n","    GROUP BY Occupation\n","\"\"\").show()"],"metadata":{"id":"p-rqUg5Ik7XQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <p style=\"color:brown;\"> 3. Occupation wise average age and monthly income  </p>\n","<p style=\"color:brown;\">Now let us calculate the average age and monthly income for each occupation. Here we have used the concept of common table expression. We have grouped the avg age and monthly income by occupation for the below results </p> \n"],"metadata":{"id":"t91mee5Elm0V"}},{"cell_type":"code","source":["ocupation_results = spark.sql(\"\"\"WITH cte AS (\n","    SELECT Occupation, ROUND(AVG(Age),2) AS avg_age, ROUND(AVG(`MonthlyIncome`),2) AS avg_monthly_income\n","    FROM travel_information_view\n","    GROUP BY Occupation\n",")\n","SELECT Occupation, avg_age, avg_monthly_income\n","FROM cte\"\"\").show()"],"metadata":{"id":"R8oBl9DEk-rG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <p style=\"color:brown;\"> 4. Occupation wise average pitch duration for products bought </p>\n","<p style=\"color:brown;\">Now let us calculate the average duration of the product pitch each occupation. Here we use the concept of subquery , so average duration is displayed by grouping occupation and gender. Result is ordered by occupdation and avg duration </p> "],"metadata":{"id":"WRwIw8q9li1W"}},{"cell_type":"code","source":["duration_result = spark.sql(\"\"\"SELECT Occupation, Gender, ROUND(AVG(DurationOfPitch),2) AS average_duration\n","FROM (\n","    SELECT Occupation, Gender, DurationOfPitch\n","    FROM travel_information_view\n","    WHERE ProdTaken = 1\n",") AS prod_taken_customers\n","GROUP BY Occupation, Gender\n","ORDER BY Occupation,average_duration DESC\"\"\").show()"],"metadata":{"id":"XbK0gNuMlB2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <p style=\"color:brown;\"> 5. Total Customers by Number of Follow-ups for Product Taken </p>\n","<p style=\"color:brown;\">Now let us calcualate total number of customers who have purchased the product, grouped by the number of follow-ups they have received.  </p> "],"metadata":{"id":"Z_uKZSx-ld5i"}},{"cell_type":"code","source":["followup_result = spark.sql(\"\"\"\n","    SELECT NumberOfFollowups, COUNT(*) AS total_customers\n","    FROM travel_information_view\n","    WHERE ProdTaken = 1\n","    GROUP BY NumberOfFollowups\n","\"\"\").show()"],"metadata":{"id":"Ohtb0TdZlFF_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <p style=\"color:brown;\"> 6. Percentage of customers who bought the package pitched. </p>\n","<p style=\"color:brown;\">Finally let us calculate the the percentage of customers who have taken a product based on the product pitched. For this we selected the product pitched, count of customers and sum of the prodtaken and then grouped by product pitched to get the necessary results </p>  \n"],"metadata":{"id":"YKPTwccDlSFm"}},{"cell_type":"code","source":["purchased_percent_result = spark.sql(\"\"\"\n","    SELECT ProductPitched, COUNT(*) AS total_customers, SUM(ProdTaken) AS customers_taken_product,\n","           ROUND((SUM(ProdTaken) / COUNT(*) * 100),2) AS percentage_taken_product\n","    FROM travel_information_view\n","    GROUP BY ProductPitched\n","    ORDER BY percentage_taken_product DESC\n","\"\"\").show()"],"metadata":{"id":"pVkUP3oilLcr"},"execution_count":null,"outputs":[]}]}
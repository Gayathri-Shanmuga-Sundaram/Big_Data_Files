{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/FQb5rUa3+kqyKrbuJvZk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fLr2KtIRadkU"},"outputs":[],"source":["# Install pyspark\n","!pip install pyspark\n","\n","# Import SparkSession\n","from pyspark.sql import SparkSession\n","\n","# Create a Spark Session\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Check Spark Session Information\n","spark\n","\n","# Import a Spark function from library\n","from pyspark.sql.functions import col\n","\n","!pip install findspark\n"]},{"cell_type":"markdown","source":["You are given housing prices for a given market (WestRoxbury.csv Download WestRoxbury.csv)\n","Create a Jupyter notebook that analyzes this data using PySpark. Load the data into a pyspark data frame and conduct any necessary datatyping/casting. In the notebook, you must answer the following questions using code. \n","1) Identify the top 10 most expensive homes.\n","2) Does remodeling a home indicate higher prices (simply compare the average between recently remodeled homes versus those that were not).\n","3) Create a linear regression model that predicts home prices using LOT SQFT, YR Built, Gross Area, Living Area, and REMODEL variables. \n","4) Using your predictive model, determine the price of a home with the following:\n","LOT SQRT: 7500\n","YR Built: 1990\n","Gross Area: 2100\n","Living Area: 1900\n","Remodel: Recent\n"],"metadata":{"id":"H1wdTrAFmSHg"}},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession;\n","\n","spark = SparkSession.builder.config(\"spark.driver.host\",\"localhost\").master(\"local[4]\").appName(\"ISM6562 Spark Assignment App\").getOrCreate();\n","\n","# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n","sc = spark.sparkContext  \n","\n","# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n","# this spark session webUI will be on a different port than the default (4040). One way to \n","# identify this part is with the following line. If there was only one spark session running, \n","# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n","spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n","print(\"Spark Session WebUI Port: \" + spark_session_port)"],"metadata":{"id":"fqsPNP8RbV5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n","sc.setLogLevel(\"ERROR\") "],"metadata":{"id":"s85ozgSQbl5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"id":"q0xju8jUbxYc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading our data into spark dataframe. "],"metadata":{"id":"K4JjIIrqmH8V"}},{"cell_type":"code","source":["# Load CSV file\n","df_spark = spark.read.csv(\"sample_data/WestRoxbury.csv\", header=True, inferSchema=True)\n","df_spark.show()"],"metadata":{"id":"c7jvVbPzb0gZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Rename Column "],"metadata":{"id":"t3VAcnUYmJI-"}},{"cell_type":"code","source":["df_renamed = df_spark.withColumnRenamed(\"TOTAL VALUE \",\"total_value\").withColumnRenamed(\"TAX\",\"tax\").withColumnRenamed(\"LOT SQFT \",\"lot_sqft\").withColumnRenamed(\"YR BUILT\",\"yr_built\").withColumnRenamed(\"GROSS AREA \",\"gross_area\").withColumnRenamed(\"LIVING AREA\",\"living_area\").withColumnRenamed(\"FLOORS \",\"floors\").withColumnRenamed(\"ROOMS\",\"rooms\").withColumnRenamed(\"BEDROOMS \",\"bedrooms\").withColumnRenamed(\"FULL BATH\",\"full_bath\").withColumnRenamed(\"HALF BATH\",\"half_bath\").withColumnRenamed(\"KITCHEN\",\"kitchen\").withColumnRenamed(\"FIREPLACE\",\"fireplace\").withColumnRenamed(\"REMODEL\",\"remodel\")\n","\n","df_renamed.show()"],"metadata":{"id":"LdiEbXP-iyVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Exploration and Transformations "],"metadata":{"id":"nP3R4YN8mEfo"}},{"cell_type":"code","source":["df_renamed.printSchema()"],"metadata":{"id":"xTrI1LNycSeH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Finding missing values"],"metadata":{"id":"71g_RjVtl_dh"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Find columns with missing values\n","columns_with_missing_values = [column for column in df_renamed.columns if df_renamed.filter(col(column).isNull()).count() > 0]\n","\n","# Print columns with missing values\n","print(\"Columns with missing values:\")\n","for column in columns_with_missing_values:\n","    print(column)\n"],"metadata":{"id":"Wu1HwNtPcYL3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imputing missing values"],"metadata":{"id":"qkT6tHv5l9Lu"}},{"cell_type":"code","source":["#from pyspark.sql.functions import col\n","\n","# Group by the column and apply the count() function\n","#count_df = df_renamed.groupBy(\"TypeofContact\").count()\n","\n","# Show the resulting counts\n","#count_df.show()"],"metadata":{"id":"bn7y9AshcbDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df_spark = df_renamed.fillna(\"Self Enquiry\", subset=[\"TypeofContact\"])"],"metadata":{"id":"3WCtKg81cbpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imputing with missing value with median value for numeric values"],"metadata":{"id":"R5oVg6uQl6N5"}},{"cell_type":"code","source":["# #from pyspark.sql.functions import col\n","# from pyspark.sql.functions import percentile_approx\n","# from pyspark.sql.functions import when\n","\n","# # Iterate over columns with missing values\n","# for column in columns_with_missing_values:\n","#     # Calculate median of the column\n","#     median_value = df_spark.select(column).agg(percentile_approx(column, 0.5)).collect()[0][0]\n","#     if median_value is not None:\n","#     # Round median_value to nearest integer\n","#         median_value_rounded = int(round(median_value))\n","#     else:\n","#         median_value_rounded = 0\n","    \n","#     # Impute missing values with median value\n","#     df_spark = df_spark.withColumn(column, when(col(column).isNull(), median_value_rounded).otherwise(col(column)))"],"metadata":{"id":"gkNAQwW7chxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # verifying if all missing values were imputed\n","# columns_with_missing_values = [column for column in df_spark.columns if df_spark.filter(col(column).isNull()).count() > 0]\n","\n","# # Print columns with missing values\n","# print(\"Columns with missing values:\")\n","# for column in columns_with_missing_values:\n","#     print(column)"],"metadata":{"id":"BOOoHrP-ckwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Save imputed data with original header\n","\n","# df_spark.write.option(\"header\", True).csv(\"/content/Big Data Output/processed_travel_withheader.csv\")"],"metadata":{"id":"ZW3TicFzcppF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_spark.show(20)"],"metadata":{"id":"lNfLqxqUc7m9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Storing the data into a persistent table and creating a temp view of the data\n"],"metadata":{"id":"dPoi2__Zl08w"}},{"cell_type":"code","source":["# Create a database\n","spark.sql(\"CREATE DATABASE IF NOT EXISTS HOUSING\")\n","\n","# Use the database\n","spark.sql(\"USE HOUSING\")"],"metadata":{"id":"d0bgjoYXknQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#spark.sql(\"DROP DATABASE HOUSING\")"],"metadata":{"id":"5H-XBDVfsZxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To drop the Table\n","#spark.sql(\"DROP TABLE house_sale\")"],"metadata":{"id":"_gPxt2IjX3Eu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store the data into a persistent table in the Travel db\n","df_renamed.write.saveAsTable(\"house_sales\")"],"metadata":{"id":"_-bVO54Okud7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a temporary view of the data\n","df_renamed.createOrReplaceTempView(\"housing_view\")"],"metadata":{"id":"hXaHacebkx8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify if the table exists in the created database\n","check = spark.sql(\"SHOW TABLES\")\n","if check.filter(check.tableName == \"housing_view\").count() > 0:\n","    print(\"Table exists in the created database.\")"],"metadata":{"id":"tRKG0Ms_k0CX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now let us find some insights by using the aggregation.\n","\n","<p style=\"color:brown;\">1) Identify the top 10 most expensive homes. </p>\n"],"metadata":{"id":"vhaboyjllqah"}},{"cell_type":"code","source":["top_exp_homes = spark.sql(\"SELECT * FROM housing_view order by total_value desc limit 10\").show()"],"metadata":{"id":"mmytc76Tk6AX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p style=\"color:brown;\">2) Does remodeling a home indicate higher prices (simply compare the average between recently remodeled homes versus those that were not).</p>\n"],"metadata":{"id":"XgsuK6U5lnto"}},{"cell_type":"code","source":["total_result = spark.sql(\"\"\"\n","    select remodel,avg(total_value) from housing_view group by remodel\n","\"\"\").show()"],"metadata":{"id":"p-rqUg5Ik7XQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3) Create a linear regression model that predicts home prices using LOT SQFT, YR Built, Gross Area, Living Area, and REMODEL variables. "],"metadata":{"id":"Gd8M4Z6BZmxq"}},{"cell_type":"markdown","source":["Let's select a the subset of columns we are interested in for our analysis."],"metadata":{"id":"A7odH5BGZxlQ"}},{"cell_type":"code","source":["df_data =df_renamed.select([\n","    'lot_sqft',\n","    'yr_built',\n","    'gross_area',\n","    'living_area',\n","    'remodel',\n","    'total_value'\n","    ]\n",")"],"metadata":{"id":"KIAX9TkNZn4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.count()"],"metadata":{"id":"vtjZxh75aPA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Remove any rows from the DataFrame that contain missing values\n","df_data = df_data.dropna()\n"],"metadata":{"id":"_QOZonl5aS6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.count()"],"metadata":{"id":"pgDtggrUan5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_data.show()"],"metadata":{"id":"X2TGtTBmassk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Training"],"metadata":{"id":"yWp4BJSTa5S9"}},{"cell_type":"markdown","source":["We earlier created the incident table with 'inferSchema' set to true. This means that the schema of the table is inferred from the data. The knowledge column was properly inferred as a boolean, however, in the pipeline below with use StringIndexer on this column - and therefore, need to convert this column data type to a string."],"metadata":{"id":"NEkg_jyva7dY"}},{"cell_type":"code","source":["df_data.printSchema()"],"metadata":{"id":"cIrwAnxZa-pY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CONVERTING BOOLEAN DATA TO NUMERICAL\n"],"metadata":{"id":"IoBkQ1c9c0g7"}},{"cell_type":"code","source":["#from pyspark.sql.types import IntegerType,BooleanType,DateType, StringType\n","\n","#df_closed_incidents = df_data.withColumn(\"knowledge\",df_data.knowledge.cast(StringType()))"],"metadata":{"id":"GDbzhGUSbExy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df_closed_incidents.printSchema()"],"metadata":{"id":"xTj-FGn3bljR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have our data ready, let's do a train test split (70/30)."],"metadata":{"id":"WhrjnOZ9botg"}},{"cell_type":"code","source":["train_data,test_data=df_data.randomSplit([0.7,0.3])"],"metadata":{"id":"Qyx3ZyMzbpf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CONVERTING STRING TO NUMERICAL"],"metadata":{"id":"xX0L58S5dR3v"}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n","# Use StringIndexer to convert the categorical columns to hold numerical data\n"," \n","remodel_indexer = StringIndexer(inputCol='remodel',outputCol='remodel_indexer',handleInvalid='keep')\n"],"metadata":{"id":"k0qoWXHKbuMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","# Vector assembler is used to create a vector of input features\n"," \n","assembler = VectorAssembler(\n","    inputCols=[\n","        \"remodel_indexer\",\n","        \"lot_sqft\",\n","        \"yr_built\",\n","        \"gross_area\",\n","        \"living_area\"   \n","    ],\n","    outputCol=\"features\"\n",")"],"metadata":{"id":"d3HRWvGObwsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","\n","# Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data\n","# in the same way as that of the train data\n","# https://spark.apache.org/docs/latest/ml-pipeline.html\n"," \n","pipe = Pipeline(stages=[\n","    remodel_indexer,\n","    assembler\n","    ]\n",")"],"metadata":{"id":"4tu8eMgGby-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fitted_pipe=pipe.fit(train_data)"],"metadata":{"id":"H4rSiFXHb1_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data=fitted_pipe.transform(train_data)\n","train_data.show()"],"metadata":{"id":"zWZuTdsOb3p0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data=fitted_pipe.transform(test_data)\n","test_data.show()"],"metadata":{"id":"9N6Z1ZOEb4FR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # For those interested in utilizing the ML/AI power of Tensorflow with Spark....\n","# # https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor\n","\n","# # In this course, we'll use the SparkML (admitedely, it's not as powerful as Tensorflow, but \n","# # it's easy to use and demonstrate ML on a Spark Cluster)\n","\n","# from pyspark.ml.regression import LinearRegression\n","\n","lr_model = LinearRegression(labelCol='total_value')\n","fit_model = lr.fit(train_data.select(['features','total_value']))\n"],"metadata":{"id":"wryqedO_b7Y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = fit_model.transform(test_data)\n","results.show()"],"metadata":{"id":"uzn2mjTSazFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results.select(['location_index','prediction']).show()"],"metadata":{"id":"oUQELeYBa1w4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate the peformance of the Linear Regression Model"],"metadata":{"id":"oa4-QoTAcEVO"}},{"cell_type":"code","source":["test_results = fit_model.evaluate(test_data)"],"metadata":{"id":"ffeCmzXfa6lZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_results.residuals.show()"],"metadata":{"id":"EH1RwHHIa8h1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n","print(f\"{'Ex Var:':7s} {test_results.explainedVariance:>7.3f}\")\n","print(f\"{'MAE:':7s} {test_results.meanAbsoluteError:>7.3f}\")\n","print(f\"{'MSE:':7s} {test_results.meanSquaredError:>7.3f}\")\n","print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n","print(f\"{'R2:':7s} {test_results.r2:>7.3f}\")"],"metadata":{"id":"hj_AqlHKa_vY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) Using your predictive model, determine the price of a home with the following:\n","LOT SQRT: 7500\n","YR Built: 1990\n","Gross Area: 2100\n","Living Area: 1900\n","Remodel: Recent\n"],"metadata":{"id":"qye0uuC_fCwE"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n","\n","from pyspark.ml.linalg import Vectors\n","\n","# Create a test dataframe with the given features\n","\n","new_home_data = [(7500, 1990, 2100, 1900, 1)]\n","\n","test_df = spark.createDataFrame(new_home_data, ['lot_sqft', 'yr_built', 'gross_area', 'living_area', 'remodel_by_index'])"],"metadata":{"id":"vhV8O0hCbCuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["home_df = assembler.transform(test_df)"],"metadata":{"id":"ChE8UYqebFXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_home_features =Vectors.dense(home_df.select('features').collect()[0][0])\n","predicted_price = fit_model.predict(new_home_features)\n"],"metadata":{"id":"rMd8KH_wbH0Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'price is  {predicted_price:.2f} dollars ' )"],"metadata":{"id":"Wr8QuzqMbLLz"},"execution_count":null,"outputs":[]}]}